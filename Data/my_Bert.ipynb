{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from time import time\n",
    "import io\n",
    "import re\n",
    "from csv import reader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.backend import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_bert_path =   '/Users/fohanlin/PycharmProjects/w266NLP/bert-master/' # change as needed \n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "# make sure that the paths are accessible within the notebook\n",
    "sys.path.insert(0,local_bert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/fohanlin/PycharmProjects/w266NLP/bert-master/',\n",
       " '/Users/fohanlin/PycharmProjects/w266NLP',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.6/lib/python36.zip',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6',\n",
       " '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/lib-dynload',\n",
       " '',\n",
       " '/Users/fohanlin/PycharmProjects/w266NLP/venv/lib/python3.6/site-packages',\n",
       " '/Users/fohanlin/PycharmProjects/w266NLP/venv/lib/python3.6/site-packages/setuptools-39.1.0-py3.6.egg',\n",
       " '/Users/fohanlin/PycharmProjects/w266NLP/venv/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/Users/fohanlin/.ipython']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/fohanlin/PycharmProjects/w266NLP/bert-master/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optimization\n",
    "import run_classifier\n",
    "import tokenization\n",
    "import run_classifier_with_tfhub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the statements\n",
    "statements = pd.read_csv(\"fomc_statement.csv\",parse_dates=[\"Date\",])\n",
    "yields = pd.read_csv(\"DailyTreasuryYield.csv\",parse_dates=[\"Date\",])\n",
    "\n",
    "# Correct one FOMC website mistake\n",
    "statements.loc[statements[\"Date\"] == datetime.date(2007, 6, 18), \"Date\"] = datetime.date(2007, 6, 28)\n",
    "statements.sort_values(by='Date', inplace=True)\n",
    "# Change a weekend to the latest business date\n",
    "statements.loc[statements[\"Date\"] == datetime.date(2010, 5, 9), \"Date\"] = datetime.date(2010, 5, 7)\n",
    "\n",
    "result = pd.merge(yields, statements, on='Date', how='left')\n",
    "result['10YR_3MO'] = result['10 YR']-result['3 MO']\n",
    "result['d3MO'] = result['3 MO'].diff()\n",
    "result['d_structure'] = result['10YR_3MO'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate rates movement for each release date\n",
    "myDates = result[result['Statement'].notnull()]['Date']\n",
    "window = 1 # business days\n",
    "for i in range(len(myDates)):\n",
    "    start_index = myDates.index[i]\n",
    "    if i == len(myDates) - 1:\n",
    "        policy_interim = np.inf\n",
    "    else: \n",
    "        policy_interim = myDates.index[i+1] - myDates.index[i]\n",
    "    sub_result = result.iloc[start_index:min(start_index+window,start_index+policy_interim)]\n",
    "    result.loc[result[\"Date\"] == myDates.iloc[i], \"3MO_move\"] = sub_result['d3MO'].sum()\n",
    "    #result.loc[result[\"Date\"] == myDates.iloc[i], \"3MO_std\"] = sub_result['d3MO'].std()\n",
    "    result.loc[result[\"Date\"] == myDates.iloc[i], \"structure_move\"] = sub_result['d_structure'].sum()\n",
    "    #result.loc[result[\"Date\"] == myDates.iloc[i], \"structure_std\"] = sub_result['d_structure'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fohanlin/PycharmProjects/w266NLP/venv/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/fohanlin/PycharmProjects/w266NLP/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "out = result[result['Statement'].notnull()]\n",
    "\n",
    "out['3MO_label'] = out['3MO_move'] > 0\n",
    "out['structure_label'] = out['structure_move'] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>1 MO</th>\n",
       "      <th>3 MO</th>\n",
       "      <th>6 MO</th>\n",
       "      <th>1 YR</th>\n",
       "      <th>2 YR</th>\n",
       "      <th>3 YR</th>\n",
       "      <th>5 YR</th>\n",
       "      <th>7 YR</th>\n",
       "      <th>10 YR</th>\n",
       "      <th>...</th>\n",
       "      <th>Statement</th>\n",
       "      <th>10YR_3MO</th>\n",
       "      <th>d3MO</th>\n",
       "      <th>d_structure</th>\n",
       "      <th>3MO_move</th>\n",
       "      <th>3MO_std</th>\n",
       "      <th>structure_move</th>\n",
       "      <th>structure_std</th>\n",
       "      <th>3MO_label</th>\n",
       "      <th>structure_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6839</th>\n",
       "      <td>2017-05-03</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.33</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.032146</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>2017-06-14</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.35</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.15</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.037859</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6897</th>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.13</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.23</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.29</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.023094</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>2017-09-20</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.04</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.32</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.28</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6965</th>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.18</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.61</td>\n",
       "      <td>1.74</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.37</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6994</th>\n",
       "      <td>2017-12-13</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.79</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.12</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.36</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.06</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>2018-01-31</td>\n",
       "      <td>1.43</td>\n",
       "      <td>1.46</td>\n",
       "      <td>1.66</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.66</td>\n",
       "      <td>2.72</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.047258</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7060</th>\n",
       "      <td>2018-03-21</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.06</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.82</td>\n",
       "      <td>2.89</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.045092</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.060828</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>2018-05-02</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.84</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.64</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.97</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.13</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.023094</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7118</th>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.94</td>\n",
       "      <td>2.09</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.71</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.98</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7152</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>1.93</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.67</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.87</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.032146</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7191</th>\n",
       "      <td>2018-09-26</td>\n",
       "      <td>2.11</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.37</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.89</td>\n",
       "      <td>2.96</td>\n",
       "      <td>3.02</td>\n",
       "      <td>3.06</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.015275</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7221</th>\n",
       "      <td>2018-11-08</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.74</td>\n",
       "      <td>2.98</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.09</td>\n",
       "      <td>3.17</td>\n",
       "      <td>3.24</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.020817</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.060828</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7247</th>\n",
       "      <td>2018-12-19</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.62</td>\n",
       "      <td>2.69</td>\n",
       "      <td>2.77</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.045826</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7274</th>\n",
       "      <td>2019-01-30</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.57</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.58</td>\n",
       "      <td>2.70</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7308</th>\n",
       "      <td>2019-03-20</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.49</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.041633</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.44</td>\n",
       "      <td>2.39</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.41</td>\n",
       "      <td>2.52</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>2019-06-19</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.96</td>\n",
       "      <td>1.74</td>\n",
       "      <td>1.70</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.03</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.043589</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7400</th>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.84</td>\n",
       "      <td>1.92</td>\n",
       "      <td>2.02</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.005774</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.043589</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>2019-09-18</td>\n",
       "      <td>1.94</td>\n",
       "      <td>1.95</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.87</td>\n",
       "      <td>1.77</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.76</td>\n",
       "      <td>1.80</td>\n",
       "      <td>...</td>\n",
       "      <td>Information received since the Federal Open Ma...</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.030551</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  1 MO  3 MO  6 MO  1 YR  2 YR  3 YR  5 YR  7 YR  10 YR  \\\n",
       "6839 2017-05-03  0.73  0.85  1.00  1.10  1.30  1.50  1.86  2.14   2.33   \n",
       "6868 2017-06-14  0.90  1.01  1.12  1.20  1.35  1.48  1.74  1.96   2.15   \n",
       "6897 2017-07-26  1.02  1.13  1.14  1.23  1.36  1.50  1.83  2.09   2.29   \n",
       "6936 2017-09-20  0.98  1.04  1.20  1.32  1.45  1.60  1.89  2.12   2.28   \n",
       "6965 2017-11-01  1.06  1.18  1.30  1.46  1.61  1.74  2.01  2.22   2.37   \n",
       "6994 2017-12-13  1.22  1.30  1.47  1.68  1.79  1.90  2.12  2.26   2.36   \n",
       "7026 2018-01-31  1.43  1.46  1.66  1.90  2.14  2.29  2.52  2.66   2.72   \n",
       "7060 2018-03-21  1.71  1.74  1.95  2.06  2.31  2.46  2.69  2.82   2.89   \n",
       "7089 2018-05-02  1.69  1.84  2.03  2.24  2.49  2.64  2.80  2.92   2.97   \n",
       "7118 2018-06-13  1.82  1.94  2.09  2.35  2.59  2.71  2.85  2.95   2.98   \n",
       "7152 2018-08-01  1.93  2.03  2.22  2.45  2.67  2.78  2.87  2.96   3.00   \n",
       "7191 2018-09-26  2.11  2.20  2.37  2.58  2.83  2.89  2.96  3.02   3.06   \n",
       "7221 2018-11-08  2.21  2.35  2.52  2.74  2.98  3.05  3.09  3.17   3.24   \n",
       "7247 2018-12-19  2.35  2.40  2.54  2.62  2.63  2.61  2.62  2.69   2.77   \n",
       "7274 2019-01-30  2.40  2.42  2.50  2.57  2.52  2.49  2.49  2.58   2.70   \n",
       "7308 2019-03-20  2.45  2.48  2.49  2.47  2.40  2.34  2.34  2.44   2.54   \n",
       "7337 2019-05-01  2.42  2.43  2.44  2.39  2.31  2.28  2.31  2.41   2.52   \n",
       "7371 2019-06-19  2.14  2.18  2.11  1.96  1.74  1.70  1.77  1.89   2.03   \n",
       "7400 2019-07-31  2.01  2.08  2.10  2.00  1.89  1.84  1.84  1.92   2.02   \n",
       "7434 2019-09-18  1.94  1.95  1.91  1.87  1.77  1.72  1.68  1.76   1.80   \n",
       "\n",
       "           ...                                                 Statement  \\\n",
       "6839       ...         Information received since the Federal Open Ma...   \n",
       "6868       ...         Information received since the Federal Open Ma...   \n",
       "6897       ...         Information received since the Federal Open Ma...   \n",
       "6936       ...         Information received since the Federal Open Ma...   \n",
       "6965       ...         Information received since the Federal Open Ma...   \n",
       "6994       ...         Information received since the Federal Open Ma...   \n",
       "7026       ...         Information received since the Federal Open Ma...   \n",
       "7060       ...         Information received since the Federal Open Ma...   \n",
       "7089       ...         Information received since the Federal Open Ma...   \n",
       "7118       ...         Information received since the Federal Open Ma...   \n",
       "7152       ...         Information received since the Federal Open Ma...   \n",
       "7191       ...         Information received since the Federal Open Ma...   \n",
       "7221       ...         Information received since the Federal Open Ma...   \n",
       "7247       ...         Information received since the Federal Open Ma...   \n",
       "7274       ...         Information received since the Federal Open Ma...   \n",
       "7308       ...         Information received since the Federal Open Ma...   \n",
       "7337       ...         Information received since the Federal Open Ma...   \n",
       "7371       ...         Information received since the Federal Open Ma...   \n",
       "7400       ...         Information received since the Federal Open Ma...   \n",
       "7434       ...         Information received since the Federal Open Ma...   \n",
       "\n",
       "      10YR_3MO  d3MO  d_structure  3MO_move   3MO_std  structure_move  \\\n",
       "6839      1.48  0.03         0.01      0.03  0.015275            0.01   \n",
       "6868      1.14  0.01        -0.07      0.01  0.000000           -0.07   \n",
       "6897      1.16 -0.05         0.01     -0.05  0.015275            0.01   \n",
       "6936      1.24  0.00         0.04      0.00  0.005774            0.04   \n",
       "6965      1.19  0.03        -0.04      0.03  0.020000           -0.04   \n",
       "6994      1.06 -0.04         0.00     -0.04  0.030000            0.00   \n",
       "7026      1.26  0.02        -0.03      0.02  0.011547           -0.03   \n",
       "7060      1.15 -0.07         0.07     -0.07  0.045092            0.07   \n",
       "7089      1.13 -0.01         0.01     -0.01  0.005774            0.01   \n",
       "7118      1.04  0.02         0.00      0.02  0.011547            0.00   \n",
       "7152      0.97  0.00         0.04      0.00  0.005774            0.04   \n",
       "7191      0.86 -0.01        -0.03     -0.01  0.015275           -0.03   \n",
       "7221      0.89 -0.02         0.04     -0.02  0.020817            0.04   \n",
       "7247      0.37  0.01        -0.06      0.01  0.010000           -0.06   \n",
       "7274      0.28  0.00        -0.02      0.00  0.005774           -0.02   \n",
       "7308      0.06  0.02        -0.09      0.02  0.026458           -0.09   \n",
       "7337      0.09  0.00         0.01      0.00  0.040000            0.01   \n",
       "7371     -0.15 -0.04         0.01     -0.04  0.005774            0.01   \n",
       "7400     -0.06  0.00        -0.04      0.00  0.005774           -0.04   \n",
       "7434     -0.15 -0.04         0.03     -0.04  0.011547            0.03   \n",
       "\n",
       "      structure_std  3MO_label  structure_label  \n",
       "6839       0.032146       True             True  \n",
       "6868       0.037859       True            False  \n",
       "6897       0.023094      False             True  \n",
       "6936       0.026458      False             True  \n",
       "6965       0.015275       True            False  \n",
       "6994       0.020817      False            False  \n",
       "7026       0.047258       True            False  \n",
       "7060       0.060828      False             True  \n",
       "7089       0.023094      False             True  \n",
       "7118       0.020817       True            False  \n",
       "7152       0.032146      False             True  \n",
       "7191       0.026458      False            False  \n",
       "7221       0.060828      False             True  \n",
       "7247       0.045826       True            False  \n",
       "7274       0.072111      False            False  \n",
       "7308       0.041633       True            False  \n",
       "7337       0.020000      False             True  \n",
       "7371       0.043589      False             True  \n",
       "7400       0.043589      False            False  \n",
       "7434       0.030551      False             True  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximal length of input 'sentences' (post tokenization).\n",
    "max_length = 500\n",
    "max_seq_length = 500\n",
    "train_text = out['Statement'].tolist()[:164]\n",
    "train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n",
    "train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n",
    "train_label = out['3MO_label'].tolist()[:164]\n",
    "\n",
    "test_text = out['Statement'].tolist()[-20:]\n",
    "test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n",
    "test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "test_label = out['3MO_label'].tolist()[-20:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow hub path to BERT module of choice\n",
    "bert_url = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        bert_module = hub.Module(bert_url)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "    return tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/fohanlin/PycharmProjects/w266NLP/bert-master/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/fohanlin/PycharmProjects/w266NLP/bert-master/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fohanlin/PycharmProjects/w266NLP/venv/lib/python3.6/site-packages/ipykernel_launcher.py:88: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=164, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Converting examples to features', max=20, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert data to InputExample format\n",
    "train_examples = convert_text_to_examples(train_text, train_label)\n",
    "test_examples = convert_text_to_examples(test_text, test_label)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels \n",
    ") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=max_seq_length)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels\n",
    ") = convert_examples_to_features(tokenizer, test_examples, max_seq_length=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_customized = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.91, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Create BERT layer, following https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b\n",
    "    init:  initialize layer. Specify various parameters regarding output types and dimensions. Very important is\n",
    "           to set the number of trainable layers.\n",
    "    build: build the layer based on parameters\n",
    "    call:  call the BERT layer within a model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_url=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_url = bert_url\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_url, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_model(max_input_length, train_layers, optimizer):\n",
    "    \"\"\"\n",
    "    Implementation of NER model\n",
    "    \n",
    "    variables:\n",
    "        max_input_length: number of tokens (max_length + 1)\n",
    "        train_layers: number of layers to be retrained\n",
    "        optimizer: optimizer to be used\n",
    "    \n",
    "    returns: model\n",
    "    \"\"\"\n",
    "    \n",
    "    in_id = tf.keras.layers.Input(shape=(max_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_length,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    \n",
    "    bert_sequence = BertLayer(n_fine_tune_layers=train_layers, pooling=\"first\")(bert_inputs)\n",
    "    \n",
    "    print(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(256, activation='relu', name='dense')(bert_sequence)\n",
    "    \n",
    "    dense = tf.keras.layers.Dropout(rate=0.1)(dense)\n",
    "    \n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid', name='textclass')(dense)\n",
    "     \n",
    "    print('pred: ', pred)\n",
    "    \n",
    "    ## Prepare for multipe loss functions, although not used here\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer_3/bert_layer_3_module_apply_tokens/bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n",
      "pred:  Tensor(\"textclass_3/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_3 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "textclass (Dense)               (None, 1)            257         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 787,713\n",
      "Non-trainable params: 109,514,298\n",
      "__________________________________________________________________________________________________\n",
      "Train on 164 samples, validate on 20 samples\n",
      "Epoch 1/5\n",
      "164/164 [==============================] - 322s 2s/sample - loss: 0.9095 - acc: 0.6524 - val_loss: 0.6378 - val_acc: 0.6500\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 322s 2s/sample - loss: 0.6060 - acc: 0.7683 - val_loss: 0.8221 - val_acc: 0.6500\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 320s 2s/sample - loss: 0.4748 - acc: 0.7988 - val_loss: 0.6331 - val_acc: 0.6500\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 321s 2s/sample - loss: 0.5109 - acc: 0.8110 - val_loss: 0.7038 - val_acc: 0.6500\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 320s 2s/sample - loss: 0.4738 - acc: 0.7988 - val_loss: 0.6804 - val_acc: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x178a7cda0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Start session\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "model = text_model(max_length + 1, train_layers=0, optimizer = adam_customized)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "    train_labels,\n",
    "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
    "    epochs=5,\n",
    "    batch_size=16#,\n",
    "    #callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(sess._closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"bert_layer_2/bert_layer_2_module_apply_tokens/bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n",
      "pred:  Tensor(\"textclass_2/Sigmoid:0\", shape=(?, 1), dtype=float32)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "textclass (Dense)               (None, 1)            257         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,302,011\n",
      "Trainable params: 787,713\n",
      "Non-trainable params: 109,514,298\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.save('BertModel.h6')\n",
    "pre_save_preds = model.predict([test_input_ids[:20], \n",
    "                               test_input_masks[:20], \n",
    "                               test_segment_ids[:20]]\n",
    "                                ) # predictions before we clear and reload model\n",
    "\n",
    "# Clear and load model\n",
    "model = None\n",
    "model = text_model(max_length + 1, train_layers=0, optimizer = adam_customized)\n",
    "initialize_vars(sess)\n",
    "model.load_weights('BertModel.h6')\n",
    "\n",
    "post_save_preds = model.predict([test_input_ids[:20],\n",
    "                                test_input_masks[:20], \n",
    "                                test_segment_ids[:20]]\n",
    "                              ) # predictions after we clear and reload model\n",
    "#all(pre_save_preds == post_save_preds) # Are they the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3230358 ],\n",
       "       [0.32791266],\n",
       "       [0.32135642],\n",
       "       [0.34893155],\n",
       "       [0.3405346 ],\n",
       "       [0.33953637],\n",
       "       [0.39936844],\n",
       "       [0.38097608],\n",
       "       [0.39637637],\n",
       "       [0.41483054],\n",
       "       [0.41104797],\n",
       "       [0.41575128],\n",
       "       [0.4108116 ],\n",
       "       [0.42024577],\n",
       "       [0.41359425],\n",
       "       [0.4108062 ],\n",
       "       [0.42806455],\n",
       "       [0.41383368],\n",
       "       [0.39492103],\n",
       "       [0.39688128]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_save_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3230358 ],\n",
       "       [0.32791266],\n",
       "       [0.32135642],\n",
       "       [0.34893155],\n",
       "       [0.3405346 ],\n",
       "       [0.33953637],\n",
       "       [0.39936844],\n",
       "       [0.38097608],\n",
       "       [0.39637637],\n",
       "       [0.41483054],\n",
       "       [0.41104797],\n",
       "       [0.41575128],\n",
       "       [0.4108116 ],\n",
       "       [0.42024577],\n",
       "       [0.41359425],\n",
       "       [0.4108062 ],\n",
       "       [0.42806455],\n",
       "       [0.41383368],\n",
       "       [0.39492103],\n",
       "       [0.39688128]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_save_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
